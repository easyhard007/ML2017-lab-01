{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.       -0.749474 -0.833214 ...,  0.       -0.82     -1.      ]\n",
      " [-1.        0.318196 -0.857143 ...,  0.       -1.       -0.9998  ]\n",
      " [ 1.       -0.611429 -0.696429 ...,  0.       -0.88     -1.      ]\n",
      " ..., \n",
      " [ 1.       -0.924812 -0.940357 ..., -1.       -0.8      -1.      ]\n",
      " [ 1.       -0.799398 -0.868929 ...,  0.       -0.9      -0.997   ]\n",
      " [ 1.       -0.781955 -0.657857 ...,  0.       -0.92     -0.994   ]]\n",
      "[-1. -1.  1.  1.  1. -1. -1.  1. -1. -1.  1. -1.  1. -1. -1. -1. -1.  1.\n",
      " -1. -1. -1.  1. -1. -1.  1.  1. -1. -1. -1.  1. -1.  1.  1. -1. -1. -1.\n",
      "  1. -1.  1. -1.  1.  1.  1. -1. -1. -1. -1.  1. -1. -1. -1.  1. -1.  1.\n",
      " -1.  1.  1. -1. -1. -1.  1.  1. -1. -1.  1.  1. -1. -1.  1. -1. -1.  1.\n",
      " -1. -1. -1.  1. -1. -1. -1.  1. -1.  1.  1. -1.  1.  1.  1.  1. -1. -1.\n",
      " -1.  1. -1.  1. -1. -1. -1.  1.  1.  1. -1.  1. -1. -1. -1.  1. -1. -1.\n",
      "  1.  1. -1.  1. -1.  1. -1. -1. -1.  1. -1.  1. -1.  1. -1.  1. -1. -1.\n",
      " -1.  1. -1.  1.  1.  1.  1.  1.  1. -1. -1.  1. -1.  1. -1.  1.  1. -1.\n",
      "  1.  1. -1. -1.  1. -1.  1.  1. -1.  1.  1.  1. -1.  1.  1. -1. -1.  1.\n",
      "  1. -1. -1. -1.  1.  1. -1. -1. -1.  1. -1.  1. -1.  1.  1. -1. -1.  1.\n",
      " -1.  1. -1. -1. -1.  1.  1.  1.  1. -1.  1. -1.  1.  1. -1. -1.  1. -1.\n",
      " -1. -1. -1.  1. -1.  1. -1.  1. -1. -1.  1.  1. -1. -1.  1.  1. -1. -1.\n",
      "  1. -1.  1.  1. -1.  1. -1. -1. -1. -1. -1. -1. -1.  1. -1. -1.  1.  1.\n",
      " -1.  1.  1. -1.  1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1.  1. -1. -1.\n",
      "  1.  1.  1. -1. -1.  1.  1. -1. -1.  1.  1. -1. -1.  1.  1.  1. -1.  1.\n",
      "  1.  1. -1.  1.  1.  1. -1.  1.  1. -1. -1.  1. -1. -1.  1.  1.  1. -1.\n",
      "  1.  1.  1. -1. -1.  1.  1.  1.  1. -1.  1.  1.  1.  1. -1.  1. -1.  1.\n",
      " -1.  1.  1.  1.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1. -1.  1. -1.\n",
      " -1.  1. -1.  1. -1.  1. -1. -1.  1.  1. -1.  1.  1. -1. -1.  1.  1.  1.\n",
      " -1.  1.  1.]\n",
      "loss: 690.0\n",
      "loss: 646.652376939\n",
      "loss: 603.304754743\n",
      "loss: 559.957133411\n",
      "loss: 516.609512943\n",
      "loss: 473.26189334\n",
      "loss: 429.933616916\n",
      "loss: 390.69915506\n",
      "loss: 369.352989924\n",
      "loss: 353.692392381\n",
      "loss: 338.322094689\n",
      "loss: 323.079743227\n",
      "loss: 308.012866959\n",
      "loss: 293.257559663\n",
      "loss: 279.592083023\n",
      "loss: 269.333004168\n",
      "loss: 262.665223492\n",
      "loss: 257.96956094\n",
      "loss: 254.660591775\n",
      "loss: 252.271338804\n",
      "loss: 250.25770073\n",
      "loss: 248.462368678\n",
      "loss: 246.775823449\n",
      "loss: 245.115500572\n",
      "loss: 243.493465658\n",
      "loss: 241.892840214\n",
      "loss: 240.298264958\n",
      "loss: 238.70974193\n",
      "loss: 237.130029062\n",
      "loss: 235.550316226\n",
      "loss: 233.977852662\n",
      "loss: 232.425195483\n",
      "loss: 230.88298748\n",
      "loss: 229.340975376\n",
      "loss: 227.825240919\n",
      "loss: 226.377211128\n",
      "loss: 224.985383252\n",
      "loss: 223.729544482\n",
      "loss: 222.544187198\n",
      "loss: 221.422108488\n",
      "loss: 220.33888954\n",
      "loss: 219.275002969\n",
      "loss: 218.250039196\n",
      "loss: 217.274515289\n",
      "loss: 216.353367019\n",
      "loss: 215.452612532\n",
      "loss: 214.606562716\n",
      "loss: 213.774904678\n",
      "loss: 212.989483846\n",
      "loss: 212.23260551\n",
      "loss: 211.520596558\n",
      "loss: 210.88622695\n",
      "loss: 210.310932013\n",
      "loss: 209.782957367\n",
      "loss: 209.292835745\n",
      "loss: 208.845416903\n",
      "loss: 208.416010821\n",
      "loss: 208.016353555\n",
      "loss: 207.66077543\n",
      "loss: 207.334083343\n",
      "loss: 207.045892384\n",
      "loss: 206.788071479\n",
      "loss: 206.55227075\n",
      "loss: 206.346395258\n",
      "loss: 206.143503597\n",
      "loss: 205.948132841\n",
      "loss: 205.754521729\n",
      "loss: 205.58651213\n",
      "loss: 205.434957237\n",
      "loss: 205.2923188\n",
      "loss: 205.162189359\n",
      "loss: 205.038545271\n",
      "loss: 204.923852165\n",
      "loss: 204.81503259\n",
      "loss: 204.718820441\n",
      "loss: 204.627428776\n",
      "loss: 204.547408866\n",
      "loss: 204.475044478\n",
      "loss: 204.40268411\n",
      "loss: 204.333049418\n",
      "loss: 204.266996119\n",
      "loss: 204.20299854\n",
      "loss: 204.138710233\n",
      "loss: 204.075920888\n",
      "loss: 204.015114287\n",
      "loss: 203.955075527\n",
      "loss: 203.897692208\n",
      "loss: 203.845310976\n",
      "loss: 203.795113553\n",
      "loss: 203.746085491\n",
      "loss: 203.697269024\n",
      "loss: 203.649021445\n",
      "loss: 203.603029381\n",
      "loss: 203.557598365\n",
      "loss: 203.51394805\n",
      "loss: 203.471153869\n",
      "loss: 203.428360323\n",
      "loss: 203.384788637\n",
      "loss: 203.34215384\n",
      "loss: 203.300500415\n",
      "loss: 203.257343024\n",
      "loss: 203.214930747\n",
      "loss: 203.173177949\n",
      "loss: 203.130722512\n",
      "loss: 203.089115011\n",
      "loss: 203.047486085\n",
      "loss: 203.006818373\n",
      "loss: 202.965809555\n",
      "loss: 202.92579351\n",
      "loss: 202.886817501\n",
      "loss: 202.847088965\n",
      "loss: 202.80855859\n",
      "loss: 202.769191845\n",
      "loss: 202.730876205\n",
      "loss: 202.693075334\n",
      "loss: 202.658619955\n",
      "loss: 202.620626783\n",
      "loss: 202.588605352\n",
      "loss: 202.551510584\n",
      "loss: 202.519051481\n",
      "loss: 202.481459985\n",
      "loss: 202.443988528\n",
      "loss: 202.413918704\n",
      "loss: 202.376659156\n",
      "loss: 202.346184441\n",
      "loss: 202.309806968\n",
      "loss: 202.279563039\n",
      "loss: 202.251156457\n",
      "loss: 202.217949175\n",
      "loss: 202.188987741\n",
      "loss: 202.16177255\n",
      "loss: 202.13335877\n",
      "loss: 202.104684249\n",
      "loss: 202.077096855\n",
      "loss: 202.04935222\n",
      "loss: 202.021832228\n",
      "loss: 201.994770461\n",
      "loss: 201.967186021\n",
      "loss: 201.940784935\n",
      "loss: 201.913052839\n",
      "loss: 201.885963054\n",
      "loss: 201.862047591\n",
      "loss: 201.835321495\n",
      "loss: 201.810144377\n",
      "loss: 201.78571524\n",
      "loss: 201.760547887\n",
      "loss: 201.736058638\n",
      "loss: 201.711516214\n",
      "loss: 201.69000296\n",
      "loss: 201.664083288\n",
      "loss: 201.641665683\n",
      "loss: 201.616574571\n",
      "loss: 201.593308558\n",
      "loss: 201.570254123\n",
      "loss: 201.546948615\n",
      "loss: 201.525795155\n",
      "loss: 201.503956671\n",
      "loss: 201.482393855\n",
      "loss: 201.460964729\n",
      "loss: 201.439350306\n",
      "loss: 201.418224291\n",
      "loss: 201.397057754\n",
      "loss: 201.376153963\n",
      "loss: 201.354464507\n",
      "loss: 201.333598649\n",
      "loss: 201.312705195\n",
      "loss: 201.291409681\n",
      "loss: 201.27108035\n",
      "loss: 201.250094871\n",
      "loss: 201.231820105\n",
      "loss: 201.210961648\n",
      "loss: 201.193681639\n",
      "loss: 201.168942276\n",
      "loss: 201.149572387\n",
      "loss: 201.13106218\n",
      "loss: 201.115234088\n",
      "loss: 201.09075079\n",
      "loss: 201.071127187\n",
      "loss: 201.051293148\n",
      "loss: 201.031732335\n",
      "loss: 201.0134303\n",
      "loss: 200.994386254\n",
      "loss: 200.974503036\n",
      "loss: 200.958309911\n",
      "loss: 200.935057119\n",
      "loss: 200.916211645\n",
      "loss: 200.897197716\n",
      "loss: 200.87898982\n",
      "loss: 200.864344715\n",
      "loss: 200.841006137\n",
      "loss: 200.825210553\n",
      "loss: 200.803193543\n",
      "loss: 200.783853504\n",
      "loss: 200.771700663\n",
      "loss: 200.751829088\n",
      "loss: 200.739881345\n",
      "loss: 200.716385895\n",
      "loss: 200.705240648\n",
      "loss: 200.69632966\n",
      "loss: 200.709409249\n",
      "[-0.00060001267698418293, 0.0067775228419069741, -0.012225468827055018, 0.021899883374127158, 0.026691977403017229, 0.0089748717721636027, 0.030003300698689211, 1.0075981469566915, 0.0069998103112857736, 0.028561199622323646, -0.0010000169170199438, 0.047399913469073678, -0.08319396498843025, 0.027916407568044848]\n",
      "total  690\n",
      "error  100\n",
      "acc rate  0.855072463768116\n"
     ]
    }
   ],
   "source": [
    "from sklearn.externals.joblib import Memory\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "eta = 0.0001\n",
    "iter = 200\n",
    "accuracy = 0.001\n",
    "lam = 0.0001 \n",
    "loss =0 \n",
    "\n",
    "m = 690\n",
    "m_train = 345\n",
    "m_test = 345\n",
    "features=14\n",
    "\n",
    "w=[0,0,0,0,0, 0,0,0,0,0, 0,0,0,0]\n",
    "grad = [0,0,0,0,0, 0,0,0,0,0, 0,0,0,0]\n",
    "yp = [0]*m\n",
    "\n",
    "iter_num = [1]*iter;\n",
    "loss_train  = [1]*iter;\n",
    "loss_test  = [1]*iter;\n",
    "\n",
    "def get_data():\n",
    "    data = load_svmlight_file(\"C:\\\\Users\\\\easyh\\\\Desktop\\\\australian_scale\",n_features=features)\n",
    "    return data[0], data[1]\n",
    "\n",
    "X, y = get_data()\n",
    "X = X.toarray()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.5, random_state=43)\n",
    "\n",
    "print(X_train)\n",
    "print(y_train)\n",
    "\n",
    "def lossAndGrad(m,X,y):\n",
    "    global loss\n",
    "    loss = 0\n",
    "    for i in range (0,m):\n",
    "        yp[i] = 0\n",
    "        for j in range(0,features):\n",
    "            yp[i] = yp[i] + X[i][j] * w[j]\n",
    "\n",
    "        if(y[i]*yp[i]-1<0):\n",
    "            loss += (1-y[i]*yp[i])\n",
    "            \n",
    "    for  j in range (0,features):\n",
    "        loss = loss + 0.5 * lam * w[j] * w[j]\n",
    "    \n",
    "    for j in range(0,features):\n",
    "        grad[j] = abs(lam*w[j])\n",
    "        for i in range(0,m):\n",
    "            if(y[i]*yp[i]-1<0):\n",
    "                grad[j] = grad[j] - y[i] * X[i][j]\n",
    "\n",
    "                \n",
    "def update():\n",
    "    for j in range(0,features):\n",
    "        w[j] = w[j] - eta*grad[j]\n",
    "        \n",
    "def train(m,X,y):\n",
    "\n",
    "    for it in range (0,iter):\n",
    "        lossAndGrad(m,X,y)\n",
    "        print(\"loss:\",loss);\n",
    "        if(loss<accuracy):\n",
    "            print(\"loss < accuracy\")\n",
    "            break\n",
    "        update()\n",
    "\n",
    "def predict(x):\n",
    "    pre = 0.0\n",
    "    for j in range (0,features):\n",
    "        pre = pre + x[j] * w[j]\n",
    "        \n",
    "    if(pre>=0) :\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "def test(X,y):\n",
    "    error = 0\n",
    "    for i in range(0,m):\n",
    "        if(predict(X[i])!=y[i]):\n",
    "            error+=1\n",
    "    print(\"total \",m)\n",
    "    print(\"error \",error)\n",
    "    print(\"acc rate \",1.0 - error/m)\n",
    "    \n",
    "train(m,X,y)\n",
    "print(w)\n",
    "\n",
    "test(X,y)\n",
    "    \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
